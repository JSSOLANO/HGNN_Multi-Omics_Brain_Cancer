{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8VmgqTAMAuj"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torch_geometric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O9JQzJwNNtPD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import HypergraphConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "02igaU3QDew4"
      },
      "outputs": [],
      "source": [
        "#UPLOAD THE MATRICES\n",
        "\n",
        "#Feacture matrix\n",
        "fecture= pd.read_csv('/content/Feature_matrix.csv', index_col='ensembl_id')\n",
        "H_target = pd.read_csv('/content/Incidence_matrix.csv',index_col='ensembl_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OgfakYvguCOq"
      },
      "outputs": [],
      "source": [
        "# Convert Dataframe to tensors\n",
        "fecture_torch = torch.tensor(fecture.values, dtype=torch.float32)\n",
        "H_target_torch = torch.tensor(H_target.values, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vPMdPSMjuRmO"
      },
      "outputs": [],
      "source": [
        "#Sparce HYperedge Index\n",
        "hyperedge_index = H_target_torch.nonzero().t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwsWaMxLYF_V",
        "outputId": "b96bad81-2de6-4f9a-ca00-f0fd4ca946c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node features shape: torch.Size([60615, 193])\n",
            "Incidence matrix shape: torch.Size([60615, 2778])\n",
            "Hyperedge index shape: torch.Size([2, 136292])\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "print(f\"Node features shape: {fecture_torch.shape}\")\n",
        "print(f\"Incidence matrix shape: {H_target_torch.shape}\")\n",
        "print(f\"Hyperedge index shape: {hyperedge_index.shape}\")\n",
        "print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uyWhtN4pYMM_"
      },
      "outputs": [],
      "source": [
        "# --- 2. The Hypergraph Autoencoder Model ---\n",
        "\n",
        "class HypergraphAutoencoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, latent_dim, num_hyperedges):\n",
        "        super(HypergraphAutoencoder, self).__init__()\n",
        "\n",
        "        # --- Encoder ---\n",
        "        # We use two HypergraphConv layers to learn node embeddings (Z_V)\n",
        "        self.encoder_layer1 = HypergraphConv(in_dim, hidden_dim)\n",
        "        self.encoder_layer2 = HypergraphConv(hidden_dim, latent_dim)\n",
        "\n",
        "        # --- Learnable Hyperedge Embeddings (Z_E) ---\n",
        "        # The encoder only gives us node embeddings. We need to also\n",
        "        # learn the embeddings for the hyperedges (Z_E) to use\n",
        "        # in our inner product decoder. We create a learnable\n",
        "        # nn.Parameter for this.\n",
        "        self.hyperedge_embeddings = nn.Parameter(\n",
        "            torch.Tensor(num_hyperedges, latent_dim)\n",
        "        )\n",
        "        # Initialize the weights (good practice)\n",
        "        nn.init.xavier_uniform_(self.hyperedge_embeddings)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def encode(self, x, hyperedge_index):\n",
        "        \"\"\" Encodes nodes into the latent space. \"\"\"\n",
        "        # Layer 1\n",
        "        z = self.encoder_layer1(x, hyperedge_index)\n",
        "        z = self.relu(z)\n",
        "        # Layer 2 (final node embeddings Z_V)\n",
        "        z_v = self.encoder_layer2(z, hyperedge_index)\n",
        "        return z_v\n",
        "\n",
        "    def decode(self, z_v, z_e):\n",
        "        \"\"\"\n",
        "        Decodes the incidence matrix H using a simple inner product.\n",
        "        This is the simplest decoder: H_hat = Z_V * Z_E^T\n",
        "        \"\"\"\n",
        "        # z_v shape: (NUM_NODES, LATENT_DIM)\n",
        "        # z_e shape: (NUM_HYPEREDGES, LATENT_DIM)\n",
        "        # We want to get (NUM_NODES, NUM_HYPEREDGES)\n",
        "        # So we multiply z_v by the transpose of z_e.\n",
        "        h_hat = torch.matmul(z_v, z_e.t())\n",
        "\n",
        "        # We return the 'logits' (raw scores) and let the loss\n",
        "        # function apply the sigmoid. This is more numerically stable.\n",
        "        return h_hat\n",
        "\n",
        "    def forward(self, x, hyperedge_index):\n",
        "        # 1. Get node embeddings (Z_V)\n",
        "        z_v = self.encode(x, hyperedge_index)\n",
        "\n",
        "        # 2. Get hyperedge embeddings (Z_E)\n",
        "        z_e = self.hyperedge_embeddings\n",
        "\n",
        "        # 3. Reconstruct the incidence matrix\n",
        "        reconstructed_h_logits = self.decode(z_v, z_e)\n",
        "\n",
        "        return reconstructed_h_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2fxnP5gJYRo9"
      },
      "outputs": [],
      "source": [
        "# Model Hyperparameters\n",
        "NODE_FEATURE_DIM = fecture_torch.shape[1]\n",
        "NUM_HYPEREDGES = H_target_torch.shape[1]\n",
        "LATENT_DIM = 10\n",
        "HIDDEN_DIM = 10\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4rScn41ugXoN"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "model = HypergraphAutoencoder(\n",
        "    in_dim=NODE_FEATURE_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    latent_dim=LATENT_DIM,\n",
        "    num_hyperedges=NUM_HYPEREDGES\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdjmaMKfgTB9",
        "outputId": "505aab9e-7595-45e5-aa36-d0f130a5f75c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 200 epochs...\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(f\"Starting training for {EPOCHS} epochs...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE1RRjipgd6j",
        "outputId": "6e298b2b-f90f-430c-b04e-d9470b8fb53a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/200], Loss: 0.5316\n",
            "Epoch [40/200], Loss: 0.3576\n",
            "Epoch [60/200], Loss: 0.1205\n",
            "Epoch [80/200], Loss: 0.0354\n",
            "Epoch [100/200], Loss: 0.0177\n",
            "Epoch [120/200], Loss: 0.0123\n",
            "Epoch [140/200], Loss: 0.0099\n",
            "Epoch [160/200], Loss: 0.0085\n",
            "Epoch [180/200], Loss: 0.0076\n",
            "Epoch [200/200], Loss: 0.0069\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # --- Forward Pass ---\n",
        "    # Get the reconstructed incidence matrix (logits)\n",
        "    h_hat_logits = model(fecture_torch, hyperedge_index)\n",
        "\n",
        "    # --- Calculate Loss ---\n",
        "    # Compare the reconstructed logits to the *original* target H matrix\n",
        "    loss = criterion(h_hat_logits, H_target_torch)\n",
        "\n",
        "    # --- Backward Pass and Optimization ---\n",
        "    # 1. Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "    # 2. Compute gradients\n",
        "    loss.backward()\n",
        "    # 3. Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ahrmv5QHjgEA"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Get the final node embeddings\n",
        "    learned_node_embeddings = model.encode(fecture_torch, hyperedge_index)\n",
        "\n",
        "    # Get the final hyperedge embeddings\n",
        "    learned_hyperedge_embeddings = model.hyperedge_embeddings\n",
        "\n",
        "    # Get the final reconstructed H (as probabilities)\n",
        "    reconstructed_h_prob = torch.sigmoid(model(fecture_torch, hyperedge_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "K4Ip12vaEzoF"
      },
      "outputs": [],
      "source": [
        "#save outputs files\n",
        "torch.save(learned_node_embeddings, 'learned_gene_embeddings.pt')\n",
        "torch.save(learned_hyperedge_embeddings, 'learned_pathway_embeddings.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwcM3LmjF5uQ"
      },
      "source": [
        "# Downstream Analysis - Clustering of Similar Genes to Most Commonly Mutated Genes in Glioma Cancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Pd759_MhGJJ7"
      },
      "outputs": [],
      "source": [
        "# Load the saved embeddings\n",
        "embeddings = torch.load('/content/learned_gene_embeddings.pt')\n",
        "\n",
        "# Convert to numpy if it's a tensor\n",
        "if isinstance(embeddings, torch.Tensor):\n",
        "    embeddings_np = embeddings.detach().cpu().numpy()\n",
        "else:\n",
        "    embeddings_np = embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5CGbjZ7xd2TG"
      },
      "outputs": [],
      "source": [
        "Genes_embedding = pd.DataFrame(embeddings_np, index=fecture.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3omHGbxaTYRC"
      },
      "source": [
        "# Find closest genes to Oncognes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QepIlTRVTboD"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import euclidean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "H7XxJTELTha6"
      },
      "outputs": [],
      "source": [
        "#Create list with Oncogenes\n",
        "List_of_Oncogenes = pd.read_csv('/content/Oncogen_Glioma.csv')['Ensembl_code'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Bm69DvpuXB3A"
      },
      "outputs": [],
      "source": [
        "def find_closest_genes_euclidean(gene_id, df_embeddings, n_closest=10):\n",
        "    \"\"\"\n",
        "    Memory-efficient function to find closest genes using Euclidean distance\n",
        "    \"\"\"\n",
        "    if gene_id not in df_embeddings.index:\n",
        "        print(f\"Gene {gene_id} not found!\")\n",
        "        return None\n",
        "\n",
        "    # Get embedding for query gene\n",
        "    query_embedding = df_embeddings.loc[gene_id].values\n",
        "\n",
        "    # Calculate distances one at a time (memory efficient)\n",
        "    distances = []\n",
        "    genes = []\n",
        "\n",
        "    for idx, gene in enumerate(df_embeddings.index):\n",
        "        if gene != gene_id:  # Skip the query gene itself\n",
        "            dist = euclidean(query_embedding, df_embeddings.iloc[idx].values)\n",
        "            distances.append(dist)\n",
        "            genes.append(gene)\n",
        "\n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame({\n",
        "        'gene': genes,\n",
        "        'euclidean_distance': distances\n",
        "    })\n",
        "\n",
        "    # Sort and get top n\n",
        "    results_df = results_df.sort_values('euclidean_distance').head(n_closest)\n",
        "\n",
        "    return results_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IueD81cidiIm"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "for gene in List_of_Oncogenes:\n",
        "    closest = find_closest_genes_euclidean(gene, Genes_embedding, n_closest=10)\n",
        "\n",
        "    if closest is not None:\n",
        "        results[gene] = closest\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"10 Closest genes to {gene} (Euclidean distance):\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(closest.to_string(index=False))\n",
        "\n",
        "# Save results\n",
        "all_results = []\n",
        "for gene, closest_genes in results.items():\n",
        "    closest_genes['query_gene'] = gene\n",
        "    all_results.append(closest_genes)\n",
        "\n",
        "final_df = pd.concat(all_results, ignore_index=True)\n",
        "final_df = final_df[['query_gene', 'gene', 'euclidean_distance']]\n",
        "final_df.to_csv('/content/all_closest_genes_euclidean.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}